{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment1-Q2.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1i0l0mESrKhmrENhcNrKtaJV21JbWyEMT","authorship_tag":"ABX9TyNyG2fEURXKxM8lYhSzJz6e"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"ZekbH8Hz9ZdM"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9UILgZdu9yzO","executionInfo":{"status":"ok","timestamp":1614784776565,"user_tz":-330,"elapsed":987,"user":{"displayName":"KASHYAPI SHUBHAM BHALCHANDRA mm16b027","photoUrl":"","userId":"13758485594999460540"}}},"source":["import numpy as np\r\n","class FeedForwardNN:\r\n","  def __init__(self, num_input_features = 100, num_hidden_layers = 3, \\\r\n","               hidden_layer_sizes = 32, num_classes = 10):\r\n","    '''\r\n","    Initializes the architecture of the Feed-Forward NN. The weights and biases\r\n","    are initialized randomly.\r\n","    It is assumed that all the parameters passed to this function are of the\r\n","    correct type (No type checking is done).\r\n","    If hidden_layer_sizes is an integer, it is assumed that all the hidden \r\n","    layers have the same size. The other option is to specify it as a list of \r\n","    integers starting with the size of hidden layer adjacent to the input layer.\r\n","    Number of neurons in the output layer = num_classes\r\n","    '''\r\n","    self.__num_input_features = num_input_features\r\n","    self.__num_hidden_layers = num_hidden_layers\r\n","    self.__hidden_layer_sizes = hidden_layer_sizes if type(hidden_layer_sizes) == list \\\r\n","                                else num_hidden_layers*[hidden_layer_sizes]\r\n","    self.__num_classes = num_classes\r\n","    self.__weights, self.__biases = [], []\r\n","    '''\r\n","    The weights are specified as a list. The i th element of the list is a 2D\r\n","    numpy array representing the weights of edges from i th to (i+1) th layer.\r\n","    Here, i = 0 is the input layer, i = 1 is the first hidden layer and \r\n","    i = (num_hidden_layers + 1) is the output layer.\r\n","    Thus, there are (num_hidden_layers + 1) numpy arrays in the list. Each of\r\n","    these arrays is initialized randomly.\r\n","    '''\r\n","    if num_hidden_layers > 0:\r\n","      self.__weights.append(np.random.randn(num_input_features, self.__hidden_layer_sizes[0]))\r\n","      for layer_no in range(num_hidden_layers-1):\r\n","        self.__weights.append(np.random.randn(self.__hidden_layer_sizes[layer_no], self.__hidden_layer_sizes[layer_no+1]))\r\n","      self.__weights.append(np.random.randn(self.__hidden_layer_sizes[num_hidden_layers-1], num_classes))\r\n","    else: # If no hidden layers\r\n","      self.__weights.append(np.random.randn(num_input_features, num_classes))\r\n","    '''\r\n","    The biases are specified as a list. The i th element of the list is a \r\n","    numpy vector representing the biases for the i th layer.\r\n","    Here, i = 0 is the first hidden layer, i = (num_hidden_layers + 1) is the\r\n","    output layer.\r\n","    Thus, there are (num_hidden_layers + 1) numpy arrays in the list. Each of\r\n","    these vectors is initialized randomly.\r\n","    '''\r\n","    for hidden_layer_size in self.__hidden_layer_sizes:\r\n","      self.__biases.append(np.random.randn(hidden_layer_size, ))\r\n","    self.__biases.append(np.random.randn(num_classes, ))\r\n","\r\n","  \r\n","  def evaluate(self, Xinput):\r\n","    '''\r\n","    Xinput should be a 2D numpy array with dtype = float. Shape along the 0th dimension should be\r\n","    num_samples and shape along the 1st dimension should be self.__num_input_features.\r\n","    The output is a 2D numpy array of having shape (num_samples, self.__num_classes)\r\n","    where each row sums to 1 and represents a probability distribution over the classes.    \r\n","    Softmax activation function is used to obtain the probability distribution in the output layer.\r\n","    Logistic activation function is used in each of the hidden layers.\r\n","    '''\r\n","    Xcompute = Xinput.copy() #Creating a copy to avoid making changes the original data\r\n","    sigmoid_func = np.vectorize(lambda x: 1/(1+np.exp(-x)))\r\n","    for Wmat, bvec in zip(self.__weights[:self.__num_hidden_layers], self.__biases[:self.__num_hidden_layers]):\r\n","      Xcompute = sigmoid_func(Xcompute.dot(Wmat) + bvec)\r\n","\r\n","    Xcompute = Xcompute.dot(self.__weights[-1]) + self.__biases[-1]\r\n","    # Computing softmax for the output layer\r\n","    Xcompute = np.exp(Xcompute) # Asuming no overflow due to exponentiation\r\n","    inv_row_sums = np.reciprocal(np.sum(Xcompute, axis = 1))\r\n","    Xcompute = np.multiply(inv_row_sums, Xcompute.T).T\r\n","    return Xcompute\r\n"],"execution_count":82,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iAHrTVGwhnla","executionInfo":{"status":"ok","timestamp":1614784950384,"user_tz":-330,"elapsed":929,"user":{"displayName":"KASHYAPI SHUBHAM BHALCHANDRA mm16b027","photoUrl":"","userId":"13758485594999460540"}},"outputId":"278f0c5e-a24f-4baa-cf59-6a7dd9ac9b06"},"source":["# Testing the Feed-Forward NN\r\n","# Test 1\r\n","nn1 = FeedForwardNN(num_input_features = 90, num_hidden_layers = 4, hidden_layer_sizes = [32,64,100,12], num_classes = 35)\r\n","Xinput1 = np.random.randn(20, 90)\r\n","Ypred1 = nn1.evaluate(Xinput1)\r\n","print('Shape of Ypred1 = {}'.format(Ypred1.shape)) # Checking if all rows sum to 1\r\n","print(np.sum(Ypred1, axis = 1))\r\n","# Test 2\r\n","nn2 = FeedForwardNN(num_input_features = 200, num_hidden_layers = 3, hidden_layer_sizes = 64, num_classes = 10)\r\n","Xinput2 = np.random.randn(30, 200)\r\n","Ypred2 = nn2.evaluate(Xinput2)\r\n","print('Shape of Ypred2 = {}'.format(Ypred2.shape)) # Checking if all rows sum to 1\r\n","print(np.sum(Ypred2, axis = 1))\r\n","# Test 3 (No hidden layer)\r\n","nn3 = FeedForwardNN(num_input_features = 50, num_hidden_layers = 0, hidden_layer_sizes = [], num_classes = 15)\r\n","Xinput3 = np.random.randn(40, 50)\r\n","Ypred3 = nn3.evaluate(Xinput3)\r\n","print('Shape of Ypred3 = {}'.format(Ypred3.shape)) # Checking if all rows sum to 1\r\n","print(np.sum(Ypred3, axis = 1))"],"execution_count":86,"outputs":[{"output_type":"stream","text":["Shape of Ypred1 = (20, 35)\n","[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","Shape of Ypred2 = (30, 10)\n","[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1.]\n","Shape of Ypred3 = (40, 15)\n","[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3bIrUPWnXRjW"},"source":[""]}]}