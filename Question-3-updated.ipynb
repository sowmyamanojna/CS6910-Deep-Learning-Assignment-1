{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "################################################\n",
    "#         Layers\n",
    "################################################\n",
    "class Input():\n",
    "    def __init__(self, data):\n",
    "        self.name = \"Input\"\n",
    "        self.input = data.reshape(-1,1)\n",
    "        self.input = np.append(self.input, 1).reshape(-1,1)\n",
    "        self.a = self.input\n",
    "        self.size = self.input.size\n",
    "\n",
    "class Dense():\n",
    "    def __init__(self, size, activation, intialization, name):\n",
    "        self.name = name\n",
    "        self.size = size\n",
    "        self.activation = activation\n",
    "        # Code for initialization\n",
    "        # self.W = \n",
    "        \n",
    "################################################\n",
    "#         Initializers\n",
    "################################################\n",
    "class RandomNormal():\n",
    "    def __init__(self, mean = 0.0, stddev = 1.0):\n",
    "        self.mean = mean\n",
    "        self.stddev = stddev\n",
    "    \n",
    "    def weights_biases(self, n_prev, n_curr):\n",
    "        W = np.random.normal(loc = self.mean, scale = self.stddev, \\\n",
    "                             size = (n_prev, n_curr))\n",
    "        b = np.random.normal(loc = self.mean, scale = self.stddev, \\\n",
    "                             size = (n_curr,))\n",
    "        return W, b\n",
    "    \n",
    "class XavierUniform():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def weights_biases(self, n_prev, n_curr):\n",
    "        upper_bound = np.sqrt(6.0/(n_prev + n_curr))\n",
    "        lower_bound = -1*upper_bound\n",
    "        W = np.random.uniform(low = lower_bound, high = upper_bound, \\\n",
    "                              size = (n_prev, n_curr))\n",
    "        b = np.zeros((n_curr,), dtype = np.float64)\n",
    "        return W, b\n",
    "    \n",
    "################################################\n",
    "#         Activations\n",
    "################################################\n",
    "class Sigmoid():\n",
    "    def __init__(self, c=1, b=0):\n",
    "        self.c = c\n",
    "        self.b = b\n",
    "\n",
    "    def value(self, x):\n",
    "        val = 1 + np.exp(-self.c*(x + self.b))\n",
    "        return 1/val\n",
    "\n",
    "    def diff(self, x):\n",
    "        y = self.value(x)\n",
    "        val = self.c*y*(1-y)\n",
    "        return val\n",
    "\n",
    "class Tanh():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def value(self, x):\n",
    "        num = np.exp(x) - np.exp(-x)\n",
    "        denom = np.exp(x) + np.exp(-x)\n",
    "        return num/denom\n",
    "\n",
    "    def diff(self, x):\n",
    "        y = self.value(x)\n",
    "        val = 1 - y**2\n",
    "        return val\n",
    "\n",
    "class Relu():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def value(self, x):\n",
    "        val = x\n",
    "        val[val<0] = 0\n",
    "        return val\n",
    "\n",
    "    def diff(self, x):\n",
    "        val = np.ones(x.shape)\n",
    "        val[val<=0] = 0\n",
    "        return val\n",
    "\n",
    "class Softmax():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def value(self, x):\n",
    "        val = np.exp(x)/np.sum(np.exp(x))\n",
    "        return val\n",
    "\n",
    "    def diff(self, x):\n",
    "        y = self.value(x)\n",
    "        # Motivation for condensed equation:\n",
    "        # https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n",
    "        val = (np.ones(y.shape) - y)\n",
    "        return val\n",
    "\n",
    "################################################\n",
    "#         Optimizers\n",
    "################################################\n",
    "class Momentum():\n",
    "    def __init__(self, eta=None, gamma=None):\n",
    "        self.update = 0\n",
    "        self.eta\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def get_update(self, W, grad):\n",
    "        self.update = self.gamma*self.update + self.eta*grad\n",
    "        W = W - self.update\n",
    "        return W\n",
    "\n",
    "class Nesterov():\n",
    "    def __init__(self, eta=None, gamma=None):\n",
    "        self.update = 0\n",
    "        self.eta = eta\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def get_update(self, W):\n",
    "        W_lookahead = W - self.gamma*self.update\n",
    "        self.update = self.gamma*self.update + self.eta*gradient(W_lookahead) # Need to call gradient function\n",
    "        W = W - self.update\n",
    "        return W\n",
    "        \n",
    "\n",
    "class AdaGrad():\n",
    "    def __init__(self, eta=1e-3, eps=1e-7):\n",
    "        self.v = 0\n",
    "        self.eta = eta\n",
    "        self.eps = eps\n",
    "    \n",
    "    def get_update(self, W, grad):\n",
    "        # eps value as in keras\n",
    "        self.v = self.v + grad**2\n",
    "        W = W - (self.eta/(self.v+self.eps)**0.5)*grad\n",
    "        return W\n",
    "\n",
    "class RMSProp():\n",
    "    def __init__(self, beta=0.9, eta = 1e-3, eps = 1e-7):\n",
    "        self.v = 0\n",
    "        self.beta = beta\n",
    "        self.eta = eta\n",
    "        self.eps = eps\n",
    "\n",
    "    def get_update(self, W, grad):\n",
    "        self.v = self.beta*self.v + (1-self.beta)*(grad**2)\n",
    "        W = W - (self.eta/(self.v+self.eps)**0.5)*grad\n",
    "        return W\n",
    "\n",
    "class Adam():\n",
    "    def __init__(self, beta1=0.9, beta2=0.999, eta=1e-3, eps=1e-7):\n",
    "        self.m = 0\n",
    "        self.v = 0\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eta = eta\n",
    "        self.eps = eps\n",
    "        self.iter = 1\n",
    "\n",
    "    def get_update(self, W, grad):\n",
    "        self.m = self.beta1*self.m + (1-self.beta1)*grad\n",
    "        self.v = self.beta2*self.v + (1-self.beta2)*(grad**2)\n",
    "        m_cap = self.m/(1-self.beta1**self.iter)\n",
    "        v_cap = self.v/(1-self.beta2**self.iter)        \n",
    "        W = W - (self.eta/(v_cap+self.eps)**0.5)*m_cap\n",
    "        self.iter += 1\n",
    "        return W\n",
    "\n",
    "class Nadam():\n",
    "    # Reference: https://ruder.io/optimizing-gradient-descent/index.html#nadam\n",
    "    def __init__(self, beta1=0.9, beta2=0.999, eta=1e-3, eps=1e-7):\n",
    "        self.m = 0\n",
    "        self.v = 0\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eta = eta\n",
    "        self.eps = eps\n",
    "        self.iter = 1\n",
    "    \n",
    "    def get_update(self, W, grad):\n",
    "        self.m = self.beta1*self.m + (1-self.beta1)*grad\n",
    "        self.v = self.beta2*self.v + (1-self.beta2)*(grad**2)\n",
    "        m_cap = self.m/(1-self.beta1**self.iter)\n",
    "        v_cap = self.v/(1-self.beta2**self.iter) \n",
    "        update = self.beta1*m_cap + ((1-self.beta1)/(1-self.beta1**self.iter))*grad\n",
    "        W = W - (self.eta/(v_cap+self.eps)**0.5)*update\n",
    "        self.iter += 1\n",
    "        return W\n",
    "        \n",
    "################################################\n",
    "#         Network\n",
    "################################################\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self, layers, loss, batch_size):\n",
    "        self.layers = layers\n",
    "        self.batch_size = batch_size\n",
    "        self.loss = loss\n",
    "\n",
    "    def forward_propogation(self, epochs):\n",
    "        X = self.layers[0].input\n",
    "        for i in range(1, len(self.layers)):\n",
    "            self.layers[i].h = self.layers[i].W @ self.layers[i-1].a + self.layers[i].b\n",
    "            self.layers[i].a = self.layers[i].activation.value(self.layers[i].h)\n",
    "\n",
    "    def backward_propogation(self):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
